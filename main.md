# Bert模型
Bert模型作为18年以来至今，一直是深度学习领域内的明星存在，其采用Transformer编码器作为核心结构，并通过大规模语料进行预训练，使得其具备了非常优秀的泛化能力，尤其是在自然语言处理领域中，仅需要使用特定下游领域的语料对Bert进行微调，便可在相应任务中得到非常好的表现。
虽然Bert已经诞生4年之久，网上也有许许多多关于Bert的见解文章，但在实际的工作和学习中还是回频繁于Bert打交道，也正是这些经历让自己意识到对Bert的了解还不够深入和清晰，和大多数朋友一样，虽然能够将Bert模型在自己的任务和数据集上跑起来，但是对于Bert模型本身仍然是存在一层神秘的面纱，如何将黑盒子打开，去明晰其中的脉络就是本项目记录的初衷。
我始终坚持认为深度学习的本质无外乎两点：1.将人类语言代替为机器语言；2. 模型的本质就是矩阵变换。
